# [GenerativeAIWithLLM](https://www.coursera.org/learn/generative-ai-with-llms/)

## Week1

### Transformer Architecture
- [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)
- [BLOOM: Big 176B Model](https://arxiv.org/pdf/2211.05100.pdf)
     - [Overview](https://bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4)
-[Vector Space Model](https://www.coursera.org/learn/classification-vector-spaces-in-nlp)

### Pre-training and scaling laws
- [Scaling laws for neural language models](https://arxiv.org/pdf/2001.08361.pdf)

### Model architectures and pre-training objectives
- [What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization](https://arxiv.org/pdf/2204.05832.pdf)
- [HuggingFace Tasks](https://huggingface.co/tasks)
- [Model Hub](https://huggingface.co/models)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)

### Scaling laws and compute-optimal models
- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
- [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)
- [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/pdf/2303.17564.pdf)

### [Hand written notes](https://drive.google.com/file/d/1MErgBiWgKKTN-6lrqdoMEu53DGT7rOSb/view?usp=sharing)