# [GenerativeAIWithLLM](https://www.coursera.org/learn/generative-ai-with-llms/)

## Week1

### [Week1 Lab](Week1/Lab_1_summarize_dialogue.ipynb)

### Transformer Architecture
- [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)
- [BLOOM: Big 176B Model](https://arxiv.org/pdf/2211.05100.pdf)
     - [Overview](https://bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4)
-[Vector Space Model](https://www.coursera.org/learn/classification-vector-spaces-in-nlp)

### Pre-training and scaling laws
- [Scaling laws for neural language models](https://arxiv.org/pdf/2001.08361.pdf)

### Model architectures and pre-training objectives
- [What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization](https://arxiv.org/pdf/2204.05832.pdf)
- [HuggingFace Tasks](https://huggingface.co/tasks)
- [Model Hub](https://huggingface.co/models)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)

### Scaling laws and compute-optimal models
- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
- [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)
- [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/pdf/2303.17564.pdf)

### [Hand written notes](https://drive.google.com/file/d/1MErgBiWgKKTN-6lrqdoMEu53DGT7rOSb/view?usp=sharing)


## Week2

### [Week2 Lab](Week2/Lab_2_summarize_dialogue.ipynb)

### Multi-task, instruction fine-tuning

- [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf)
- [Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning](https://blog.research.google/2021/10/introducing-flan-more-generalizable.html)

### Model Evaluation Metrics

- [HELM - Holistic Evaluation of Language Models](https://crfm.stanford.edu/helm/latest/)
- [General Language Understanding Evaluation (GLUE) benchmark](https://openreview.net/pdf?id=rJ4km2R5t7)
- [SuperGLUE](https://super.gluebenchmark.com/)
- [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013.pdf)
- [Measuring Massive Multitask Language Understanding (MMLU)](https://arxiv.org/pdf/2009.03300.pdf)
- [BigBench-Hard - Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models](https://arxiv.org/pdf/2206.04615.pdf)

### Parameter- efficient fine tuning (PEFT)
- [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.15647.pdf)
- [On the Effectiveness of Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2211.15583.pdf)

### LoRA
- [LoRA Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)

### Prompt tuning with soft prompts
- [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)